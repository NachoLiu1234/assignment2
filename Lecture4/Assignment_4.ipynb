{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=128, activation='relu', input_dim=13))\n",
    "model.add(Dense(units=32, activation='relu', input_dim=128))\n",
    "model.add(Dense(units=8, activation='sigmoid', input_dim=32))\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 0s 275us/step - loss: 222.2574 - mse: 222.2574\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 0s 48us/step - loss: 87.0786 - mse: 87.0786\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 84.5962 - mse: 84.5962\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 0s 42us/step - loss: 84.5115 - mse: 84.5115\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.6511 - mse: 84.6511\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.6275 - mse: 84.6275\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 0s 28us/step - loss: 84.7575 - mse: 84.7574\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.7367 - mse: 84.7367\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.6946 - mse: 84.6946\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.6950 - mse: 84.6950\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.6390 - mse: 84.6389\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.8210 - mse: 84.8210\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.6674 - mse: 84.6674\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.7045 - mse: 84.7045\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.7176 - mse: 84.7176\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.5453 - mse: 84.5453\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.7022 - mse: 84.7022\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.5654 - mse: 84.5654\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.6281 - mse: 84.6281\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.5692 - mse: 84.5692\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.7652 - mse: 84.7652\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.6401 - mse: 84.6401\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.5544 - mse: 84.5544\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 84.7032 - mse: 84.7032\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.8815 - mse: 84.8815\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.6991 - mse: 84.6991\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.6149 - mse: 84.6149\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.6013 - mse: 84.6013\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s 53us/step - loss: 84.7893 - mse: 84.7893\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.5679 - mse: 84.5679\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.6093 - mse: 84.6093\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.6257 - mse: 84.6257\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.8385 - mse: 84.8385\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.5602 - mse: 84.5602\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.6257 - mse: 84.6257\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.5131 - mse: 84.5130\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.6360 - mse: 84.6360\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s 42us/step - loss: 84.5865 - mse: 84.5865\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.8021 - mse: 84.8021\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.8416 - mse: 84.8416\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 84.5400 - mse: 84.5400\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.6824 - mse: 84.6824\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 84.8121 - mse: 84.8121\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.8280 - mse: 84.8280\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.7813 - mse: 84.7813\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.5482 - mse: 84.5481\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 84.6944 - mse: 84.6944\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.6224 - mse: 84.6224\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.5704 - mse: 84.5704\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.5490 - mse: 84.5490\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.6530 - mse: 84.6530\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 84.8035 - mse: 84.8035\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.6214 - mse: 84.6214\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.5536 - mse: 84.5536\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 84.7030 - mse: 84.7030\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.6270 - mse: 84.6270\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 0s 27us/step - loss: 84.6506 - mse: 84.6506\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.7266 - mse: 84.7266\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.6072 - mse: 84.6072\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.7036 - mse: 84.7036\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 84.5914 - mse: 84.5914\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.7331 - mse: 84.7332\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 84.7170 - mse: 84.7170\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.8542 - mse: 84.8542\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.5178 - mse: 84.5178\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.6152 - mse: 84.6152\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 84.6412 - mse: 84.6412\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.6868 - mse: 84.6868\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 84.7259 - mse: 84.7259\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.5718 - mse: 84.5718\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.7511 - mse: 84.7511\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.6628 - mse: 84.6628\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 84.7515 - mse: 84.7515\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.5929 - mse: 84.5929\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 84.7589 - mse: 84.7589\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 85.1066 - mse: 85.1066\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 84.7593 - mse: 84.7593\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 84.6287 - mse: 84.6287\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8055 - mse: 84.8055\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 84.7369 - mse: 84.7369\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.8355 - mse: 84.8355\n",
      "Epoch 82/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.6768 - mse: 84.6768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.6642 - mse: 84.6642\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.8382 - mse: 84.8382\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.5546 - mse: 84.5546\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 84.7605 - mse: 84.7605\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.6063 - mse: 84.6063\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.7608 - mse: 84.7608\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.6716 - mse: 84.6716\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 84.7481 - mse: 84.7481\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 84.6742 - mse: 84.6742\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 84.6399 - mse: 84.6399\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.7362 - mse: 84.7362\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s 28us/step - loss: 84.5229 - mse: 84.5230\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 110.8960 - mse: 110.896 - 0s 35us/step - loss: 84.5308 - mse: 84.5308\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 84.4430 - mse: 84.4430\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.7085 - mse: 84.7085\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 84.5301 - mse: 84.5301\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.5632 - mse: 84.5632\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.6106 - mse: 84.6106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2325bf1cd30>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'], )\n",
    "model.fit(X_, y_, batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 0s 149us/step - loss: 172.0894 - mse: 172.0894\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 82.2804 - mse: 82.2804\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 81.4397 - mse: 81.4397\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 78.8897 - mse: 78.8897\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 0s 47us/step - loss: 79.7426 - mse: 79.7426\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 77.2244 - mse: 77.2244\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 76.7227 - mse: 76.7227\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 78.8061 - mse: 78.8061\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 76.7265 - mse: 76.7265\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 0s 48us/step - loss: 76.0321 - mse: 76.0322\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 73.9327 - mse: 73.9327\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.3768 - mse: 84.3768\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 80.3431 - mse: 80.3431\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 79.6966 - mse: 79.6966\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 80.1989 - mse: 80.1989\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s 42us/step - loss: 80.5798 - mse: 80.5798\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 0s 48us/step - loss: 80.6897 - mse: 80.6897\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s 64us/step - loss: 80.4791 - mse: 80.4791\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 80.2435 - mse: 80.2435\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s 28us/step - loss: 81.6448 - mse: 81.6448\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 79.7350 - mse: 79.7350\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 79.7183 - mse: 79.7183\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 79.3503 - mse: 79.3503\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 81.1604 - mse: 81.1604\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 78.9388 - mse: 78.9388\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 78.8279 - mse: 78.8279\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 58.5973 - mse: 58.597 - 0s 41us/step - loss: 78.7682 - mse: 78.7682\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 44us/step - loss: 79.2207 - mse: 79.2207\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 78.1870 - mse: 78.1870\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 76.7887 - mse: 76.7887\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s 56us/step - loss: 78.5314 - mse: 78.5314\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 79.6708 - mse: 79.6708\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 57us/step - loss: 78.0154 - mse: 78.0154\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 0s 41us/step - loss: 78.6413 - mse: 78.6413\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 42us/step - loss: 77.5756 - mse: 77.5756\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 78.4219 - mse: 78.4219\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 77.5776 - mse: 77.5776\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 78.9441 - mse: 78.9441\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 77.5521 - mse: 77.5521\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 59us/step - loss: 77.8472 - mse: 77.8472\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 78.7610 - mse: 78.7610\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 79.7521 - mse: 79.7521\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 78.9617 - mse: 78.9617\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 77.6171 - mse: 77.6171\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 77.8991 - mse: 77.8991\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 77.7808 - mse: 77.7808\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 78.2033 - mse: 78.2033\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 78.0971 - mse: 78.0971\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 79.1242 - mse: 79.1242\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 0s 43us/step - loss: 78.0519 - mse: 78.0519\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 77.6306 - mse: 77.6306\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 77.5013 - mse: 77.5013\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s 42us/step - loss: 77.4442 - mse: 77.4442\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 77.4251 - mse: 77.4251\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 78.9166 - mse: 78.9166\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 78.1356 - mse: 78.1356\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 0s 42us/step - loss: 78.8162 - mse: 78.8162\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 77.8991 - mse: 77.8991\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 0s 32us/step - loss: 77.1855 - mse: 77.1855\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 77.6708 - mse: 77.6708\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 79.3892 - mse: 79.3892\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s 68us/step - loss: 78.0568 - mse: 78.0568\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s 39us/step - loss: 79.2176 - mse: 79.2176\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 77.9456 - mse: 77.9456\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s 36us/step - loss: 78.3708 - mse: 78.3708\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 77.5152 - mse: 77.5152\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s 52us/step - loss: 77.8821 - mse: 77.8821\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 78.4779 - mse: 78.4779\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 79.0660 - mse: 79.0660\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 79.3394 - mse: 79.3394\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 78.2894 - mse: 78.2894\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 78.6185 - mse: 78.6185\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 77.7676 - mse: 77.7676\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 77.2174 - mse: 77.2174\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 77.4914 - mse: 77.4914\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 77.6945 - mse: 77.6945\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s 28us/step - loss: 77.8984 - mse: 77.8984\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 78.1841 - mse: 78.1841\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 77.5975 - mse: 77.5975\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 75.7686 - mse: 75.7686\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 74.8911 - mse: 74.8911\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 30us/step - loss: 80.6460 - mse: 80.6460\n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s 31us/step - loss: 79.9416 - mse: 79.9416\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 79.2155 - mse: 79.2155\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 59.4232 - mse: 59.423 - 0s 45us/step - loss: 78.3824 - mse: 78.3824\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 79.2238 - mse: 79.2238\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 0s 34us/step - loss: 79.5733 - mse: 79.5733\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s 46us/step - loss: 82.1653 - mse: 82.1653\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 0s 28us/step - loss: 78.4985 - mse: 78.4985\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 79.1717 - mse: 79.1717\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 0s 27us/step - loss: 77.4756 - mse: 77.4756\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 80.3498 - mse: 80.3498\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 0s 29us/step - loss: 78.0753 - mse: 78.0753\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s 37us/step - loss: 77.7945 - mse: 77.7945\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 0s 40us/step - loss: 87.6564 - mse: 87.6564\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 0s 30us/step - loss: 86.8672 - mse: 86.8672\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 0s 38us/step - loss: 85.6857 - mse: 85.6857\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.8701 - mse: 84.8701\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 84.5206 - mse: 84.5206\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 0s 33us/step - loss: 84.7059 - mse: 84.7059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2325c0835c0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])\n",
    "model.fit(X_, y_, batch_size=32, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算前向传播的loss值, 和反向传播的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为如果多层神经元使用wx + b, 最后的结果也是wx + b, 最后的w是w[1]乘上w[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归的损失函数, 是-ylog(y_hat) -(1-y)log(1-y_hat)这个是二分类的, 多分类的是-sigma(y*log(y_hat))\n",
    "老师, 我想问一下, 是不是还有一个是线性回归, 线性回归是mse和绝对值的啊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c, ab是放在中间的隐层神经元中的激活函数, d是-1到1的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一开始的初始值都是相同的, 之后根据梯度求到的偏导数也都是相同的, 最后迭代的梯度下降的方向也是一样的, 最后所有的神经元的w和b都会是一个值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5792640357111"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def loss_function(z_list, label_list):\n",
    "    def calculate_softmax(i):\n",
    "        return np.e ** (z_list[i]) / np.sum([np.e ** z for z in z_list])\n",
    "    loss = -np.sum([label_list[i]*np.log(calculate_softmax(i)) for i in range(len(label_list))])\n",
    "    return loss\n",
    "\n",
    "z_list = [0.1,0.1,0.1,0.4, 0.1,0.6,0.1]\n",
    "label_list = [0,0,0,0, 0,1,0]\n",
    "loss_function(z_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = None\n",
    "    _z = np.e ** z\n",
    "    s =  _z/ (_z + 1)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.randn(dim, 1)\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    def calculate_z(w, b, X):\n",
    "        return np.dot(w.T, X) + b\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(calculate_z(w, b, X))\n",
    "    cost = -np.average(\n",
    "        np.sum(Y*np.log(A))\n",
    "    )\n",
    "    \n",
    "    dw = None\n",
    "    db = None\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = None\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = None\n",
    "        b = None\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = None\n",
    "    \n",
    "    for i in range(A.shape[i]):\n",
    "        None \n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_trein, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digits (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
